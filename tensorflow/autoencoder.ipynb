{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "basic_autoencoder.ipynb\n",
    "\n",
    "A very basic autoencoder in tensorflow to recognize (but not classify) MNIST digits. \n",
    "The encoder computes the code  $\\textbf{h} = \\sigma(\\mathbf{W_{e}}\\mathbf{x} + \\text{b})$, while the decoder computes the reconstruction \n",
    "$\\textbf{r} = \\sigma(\\mathbf{W_{d}}\\textbf{h} + \\text{b})$, \n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. Finally, the loss function (Mean Squared Error) is $L(\\textbf{x},\\textbf{r}) = \\frac{1}{N}\\sum_{i=1}^{N}(x^{(i)} - r^{(i)})^2$.\n",
    "\n",
    "Perhaps interestingly, messing with training_epochs \n",
    "and training_batch_size gives insight into how training proceess \n",
    "(training_batch_size around 1 not surprisingly yields random noise, ...) \n",
    "\n",
    "\n",
    "David Meyer     <br \\>\n",
    "<dmm@1-4-5.net> <br \\>\n",
    "Tue Aug 23 13:35:00 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from   __future__                          import division, print_function, absolute_import \n",
    "from   tensorflow.examples.tutorials.mnist import input_data \n",
    "from   datetime                            import timedelta \n",
    "import tensorflow                          as     tf \n",
    "import numpy                               as     np \n",
    "import matplotlib.pyplot                   as     plt \n",
    "import time \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       global parameters \n",
    "# \n",
    "\n",
    "DEBUG               = 1                         # more debug \n",
    "USE_REGULARIZER     = 1                         # use regularization? \n",
    "learning_rate       = 0.01 \n",
    "test_batch_size     = 256 \n",
    "display_step        = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       MNIST parameters \n",
    "# \n",
    "img_size            = 28                        # images 28 x 28 \n",
    "img_size_flat       = img_size * img_size       # flattened \n",
    "img_shape           = (img_size, img_size)      # shape \n",
    "num_channels        = 1                         # 1 is greyscale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  \n",
    "#       Network Parameters \n",
    "# \n",
    "n_input             = img_size_flat             # input size (img shape: 28*28) \n",
    "# \n",
    "#       rule of thumb: n_hidden = n_input/3 \n",
    "# \n",
    "#n_hidden            = int(n_input/3)  \n",
    "n_hidden            = 100                       # testing \n",
    "num_classes         = 10                        # not used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "#       get MNIST data set and place holder for feed_dict \n",
    "# \n",
    "data  = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) \n",
    "X     = tf.placeholder(\"float\", [None, n_input]) \n",
    "# \n",
    "#       one-hot encoded class labels [0-9, one hot encoded] \n",
    "# \n",
    "data.test.cls = np.argmax(data.test.labels, axis=1) \n",
    "# \n",
    "#       get the images \n",
    "# \n",
    "images = data.test.images[0:9] \n",
    "# \n",
    "#       Get the true classes for those images (again, autoencoder, not \n",
    "#       really using these, execpt for checking that we loaded MNIST \n",
    "#       correctly (see plot_images below) \n",
    "# \n",
    "cls_true = data.test.cls[0:9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       weights and biases \n",
    "# \n",
    "#       Note: one hidden layer. Thi sapproach is cool as it is easily \n",
    "#       generalized to many hidden layers. \n",
    "# \n",
    "# \n",
    "weights = { \n",
    "    'encoder': tf.Variable(tf.random_normal([n_input, n_hidden])), \n",
    "    'decoder': tf.Variable(tf.random_normal([n_hidden, n_input])) \n",
    "} \n",
    "biases = { \n",
    "    'encoder': tf.Variable(tf.random_normal([n_hidden])), \n",
    "    'decoder': tf.Variable(tf.random_normal([n_input])), \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       encoder/decoder \n",
    "#        \n",
    "#       try tf.nn.sigmoid, tf.nn.relu, etc for nonlinearity \n",
    "#       nonlinearity=False means transfer function (aka activation funtion) \n",
    "#       g(x) = x \n",
    "# \n",
    "def encoder(x, nonlinearity=False): \n",
    "    code = tf.add(tf.matmul(x, weights['encoder']), biases['encoder']) \n",
    "    if nonlinearity: \n",
    "        code = nonlinearity(code) \n",
    "    return code \n",
    " \n",
    "def decoder(code, nonlinearity=False): \n",
    "    reconstruction = tf.add(tf.matmul(code, weights['decoder']), biases['decoder']) \n",
    "    if nonlinearity: \n",
    "        reconstruction = nonlinearity(reconstruction) \n",
    "    return reconstruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       get the encoding and decoding operations \n",
    "# \n",
    "#       relu seems less efficient here \n",
    "# \n",
    "# \n",
    "#       first encode \n",
    "# \n",
    "encoder_op = encoder(X,tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       then decode \n",
    "# \n",
    "decoder_op = decoder(encoder_op,tf.nn.sigmoid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       decoder_op is our predicted value (y_pred) \n",
    "# \n",
    "y_pred = decoder_op \n",
    "# \n",
    "#       y_true is the input X  \n",
    "# \n",
    "y_true = X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "#       with regularization \n",
    "# \n",
    "#       cost = tf.add(tf.reduce_mean(tf.pow(y_true - y_pred, 2)), \n",
    "#                     tf.mul(reg_constant,tf.reduce_sum(reg_losses))) \n",
    "# \n",
    "# Epoch: 0001 cost = 0.455753744 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 1, elapsed time: 0:00:00) \n",
    "# Epoch: 0001 cost = 0.189976141 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 9, elapsed time: 0:00:00) \n",
    "# Epoch: 0001 cost = 0.071388490 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 90, elapsed time: 0:00:02) \n",
    "# Epoch: 0001 cost = 0.028894797 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 900, elapsed time: 0:00:30) \n",
    "#       w/o regularization \n",
    "# \n",
    "#       cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) \n",
    "# \n",
    "# Epoch: 0001 cost = 0.431725562 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 1, elapsed time: 0:00:00) \n",
    "# Epoch: 0001 cost = 0.190942019 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 9, elapsed time: 0:00:00) \n",
    "# Epoch: 0001 cost = 0.072323456 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 90, elapsed time: 0:00:02) \n",
    "# Epoch: 0001 cost = 0.034361549 \n",
    "# Optimization Finished...(training_epochs: 1,training_batch_size: 900, elapsed time: 0:00:30) \n",
    "# \n",
    "# \n",
    "reg_losses   = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) \n",
    "reg_constant = 0.01  \n",
    "# \n",
    "# \n",
    "if (USE_REGULARIZER): \n",
    "        error = tf.add(tf.reduce_mean(tf.square(tf.sub(y_true,y_pred))), \n",
    "                       tf.mul(reg_constant,tf.reduce_sum(reg_losses))) \n",
    "else: \n",
    "        error = tf.reduce_mean(tf.square(tf.sub(y_true,y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       use the Adam optimizer \n",
    "# \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(error) \n",
    "# \n",
    "#       might try others, e.g.,  \n",
    "# \n",
    "#       optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(error) \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "#       Get tensorlfow going \n",
    "# \n",
    "init = tf.initialize_all_variables() \n",
    "# \n",
    "#       Launch the graph (use InteractiveSession as that is more convenient while using Notebooks) \n",
    "# \n",
    "session = tf.InteractiveSession() \n",
    "session.run(init) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       check that we loaded MNIST correctly \n",
    "# \n",
    "def plot_images(images, cls_true, cls_pred=None): \n",
    "    assert len(images) == len(cls_true) == 9 \n",
    "    fig, ax = plt.subplots(3, 3) \n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3) \n",
    "    for i, ax in enumerate(ax.flat): \n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary') \n",
    "        if cls_pred is None: \n",
    "            xlabel = \"Label: {0}\".format(cls_true[i]) \n",
    "        else: \n",
    "            xlabel = \"Label: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i]) \n",
    "        ax.set_xlabel(xlabel) \n",
    "        ax.set_xticks([])               # get rid of ticks \n",
    "        ax.set_yticks([]) \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       break up into batches and run the optimizer \n",
    "# \n",
    "def optimize(training_epochs,training_batch_size): \n",
    "        start_time = time.time() \n",
    "        for epoch in range(training_epochs): \n",
    "            for i in range(training_batch_size):   # Loop over all batches \n",
    "                batch_xs, _ = data.train.next_batch(training_batch_size) \n",
    "                _, c = session.run([optimizer, error], feed_dict={X: batch_xs}) \n",
    "            if epoch % display_step == 0: \n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"error =\", \"{:.9f}\".format(c)) \n",
    "        end_time = time.time() \n",
    "        time_dif = end_time - start_time \n",
    "        print('Optimization Finished...training_epochs: {:d},training_batch_size: {:d}, elapsed time: {:s}' \n",
    "              .format(training_epochs,training_batch_size,str(timedelta(seconds=int(round(time_dif)))))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "#       display_reconstruction \n",
    "# \n",
    "#       Run the encoder/decoder on the test set, compare originals to reconstruction \n",
    "# \n",
    "#       Compare original images with their test set reconstructions \n",
    "# \n",
    "# \n",
    "def display_reconstruction(examples_to_show,fontsize): \n",
    "        reconstruction = session.run(y_pred, \n",
    "                                     feed_dict={X: data.test.images[:examples_to_show]}) \n",
    "        fig, ax = plt.subplots(2,10,figsize=(10,3)) \n",
    "        plt.style.use('grayscale')                                      # MNIST is depth 1 \n",
    "        aboutmiddle = int((examples_to_show/2) - 1)                     # sort of \n",
    "        ax[0][aboutmiddle].set_title('MNIST',                           # move over then center \n",
    "                           horizontalalignment='center', \n",
    "                           fontsize=fontsize)   \n",
    "        ax[1][aboutmiddle].set_title('Reconstruction',                  # move over then center \n",
    "                           horizontalalignment='center', \n",
    "                           fontsize=fontsize) \n",
    "        for i in range(examples_to_show): \n",
    "            ax[0][i].set_xticks([])                                     # has to be a better way \n",
    "            ax[0][i].set_yticks([])                                     # ... \n",
    "            ax[1][i].set_xticks([])                                     # ... \n",
    "            ax[1][i].set_yticks([])                                     # still removing ticks \n",
    "            ax[0][i].imshow(np.reshape(data.test.images[i],(28, 28))) \n",
    "            ax[1][i].imshow(np.reshape(reconstruction[i],  (28, 28))) \n",
    "       # fig.show() \n",
    "        plt.show()\n",
    "        plt.draw() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "training_epochs = 10                            # arbitrary \n",
    "# \n",
    "# \n",
    "#       doesn't quite display right, but ... \n",
    "# \n",
    "for batch_size in [1, 9, 90, 1000]:              # 1+9 = 10, 10+90 = 100, ...= 1000 \n",
    "        optimize(training_epochs=training_epochs,training_batch_size=batch_size) \n",
    "        display_reconstruction(examples_to_show=10,fontsize=18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
