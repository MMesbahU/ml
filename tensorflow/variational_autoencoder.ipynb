{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "variational_autoencoder.ipynb \n",
    " \n",
    "See http://arxiv.org/abs/1312.6114 and http://www.1-4-5.net/~dmm/ml/vae.pdf for an \n",
    "introduction to VAEs. \n",
    " \n",
    "Inspiration from https://arxiv.org/abs/1312.6114 and https://jmetzen.github.io/2015-11-27/vae.html \n",
    "\n",
    "David Meyer \n",
    "<dmm@1-4-5.net>\n",
    "Thu Oct  6 10:07:30 2016 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# \n",
    "#       Get the libraries we need \n",
    "from   tensorflow.examples.tutorials.mnist import input_data \n",
    "from   sklearn.metrics                     import mean_squared_error \n",
    "from   datetime                            import timedelta \n",
    "import tensorflow                          as     tf \n",
    "import numpy                               as     np \n",
    "import matplotlib.pyplot                   as     plt \n",
    "import time \n",
    "import math \n",
    "#\n",
    "#\n",
    "import math \n",
    "# \n",
    "# \n",
    "#       global parameters \n",
    "# \n",
    "DEBUG               = 1 \n",
    "training_epochs     = 1         # use 1 for debugging \n",
    "display_step        = 1         # ditto\n",
    "# \n",
    "# \n",
    "#       initialize random number generators \n",
    "# \n",
    "np.random.seed(0) \n",
    "tf.set_random_seed(0) \n",
    "# \n",
    "#       get MNIST. Store it on ./MNIST_data \n",
    "# \n",
    "mnist     = input_data.read_data_sets('MNIST_data', one_hot=True) \n",
    "n_samples = mnist.train.num_examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're using Xavier initialization of weights and biases. See https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow on how to do \n",
    "xavier init in tensorflow. See http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1):  \n",
    "    low  = -constant * np.sqrt(6.0/(fan_in + fan_out))  \n",
    "    high =  constant * np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    return tf.random_uniform((fan_in, \n",
    "                              fan_out), \n",
    "                              minval=low, \n",
    "                              maxval=high, \n",
    "                              dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a VariationalAutoencoder class so we can instantiate it various ways. \n",
    "\n",
    "Based on D. Kingma and Welling, M., \"Auto-Encoding Variational Bayes\", https://arxiv.org/abs/1312.6114. \n",
    "\n",
    "The trained model can be used to reconstruct unseen input, to generate new samples, and to map \n",
    "inputs to the latent space. \n",
    "\n",
    "Again, inspiration from https://jmetzen.github.io/2015-11-27/vae.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object): \n",
    "    def __init__(self, \n",
    "                 network_architecture, \n",
    "                 transfer_fct=tf.nn.softplus,  \n",
    "                 learning_rate=0.001, \n",
    "                 batch_size=100): \n",
    "        self.network_architecture = network_architecture \n",
    "        self.transfer_fct         = transfer_fct \n",
    "        self.learning_rate        = learning_rate \n",
    "        self.batch_size           = batch_size \n",
    "         \n",
    "        self.x = tf.placeholder(tf.float32,     # input placeholder \n",
    "                                [None, network_architecture[\"n_input\"]]) \n",
    "        self._create_network()                   \n",
    "# \n",
    "#       Define loss function based variational upper-bound \n",
    "#       and corresponding optimizer  \n",
    "# \n",
    "        self._elbo() \n",
    "# \n",
    "#       set up tensorflow \n",
    "# \n",
    "        init = tf.initialize_all_variables()    # init variables \n",
    "        self.sess = tf.InteractiveSession()      \n",
    "        self.sess.run(init)                     # launch the session \n",
    "# \n",
    "#       create autoencoder network \n",
    "#     \n",
    "    def _create_network(self): \n",
    "# \n",
    "#       First initialize weights and biases \n",
    "# \n",
    "        network_weights = self._initialize_weights(**self.network_architecture) \n",
    "# \n",
    "#       f(x;\\phi) returns the mean and variance of a Gaussian that can be \n",
    "#       used to sample the hidden variables z. That is, \n",
    "#       z ~ q_{\\phi}(z|x) = q(z;f(x,\\phi)) \\approx N(z| f(x,\\phi)), See \n",
    "#       e.g., http://www.1-4-5.net/~dmm/ml/vae.pdf \n",
    "# \n",
    "        self.z_mean, self.z_log_sigma_sq = self._f(network_weights[\"weights_f\"],network_weights[\"biases_f\"]) \n",
    "# \n",
    "# \n",
    "#       draw one sample z from N(0,1) \n",
    "# \n",
    "        n_z = self.network_architecture[\"n_z\"] \n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, dtype=tf.float32) \n",
    "# \n",
    "#       compute z = mu + sigma*epsilon (reparameterization trick) \n",
    "# \n",
    "        self.z = tf.add(self.z_mean,tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps)) \n",
    "# \n",
    "#       The generator outputs the mean of the Bernoulli distribution \n",
    "#       of the reconstructed input \n",
    "# \n",
    "        self.x_reconstr_mean = self._g(network_weights[\"weights_g\"],network_weights[\"biases_g\"]) \n",
    "\n",
    "        # \n",
    "#       housekeeping/weights and biases data structures \n",
    "# \n",
    "#       store these in the dictionary \"dict\" \n",
    "# \n",
    "# \n",
    "    def _initialize_weights(self, \n",
    "                            n_hidden_f_1, \n",
    "                            n_hidden_f_2,  \n",
    "                            n_hidden_g_1, \n",
    "                            n_hidden_g_2,  \n",
    "                            n_input, n_z): \n",
    "        all_weights = dict() \n",
    "        all_weights['weights_f'] = { \n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_f_1)), \n",
    "            'h2': tf.Variable(xavier_init(n_hidden_f_1, n_hidden_f_2)), \n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_f_2, n_z)), \n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_f_2, n_z))} \n",
    "        all_weights['biases_f'] = { \n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_f_1], dtype=tf.float32)), \n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_f_2], dtype=tf.float32)), \n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)), \n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))} \n",
    "        all_weights['weights_g'] = { \n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_g_1)), \n",
    "            'h2': tf.Variable(xavier_init(n_hidden_g_1, n_hidden_g_2)), \n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_g_2, n_input)), \n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_g_2, n_input))} \n",
    "        all_weights['biases_g'] = { \n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_g_1], dtype=tf.float32)), \n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_g_2], dtype=tf.float32)), \n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)), \n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))} \n",
    "        return all_weights \n",
    "    \n",
    "# \n",
    "#       _f \n",
    "# \n",
    "#       Probabilistic encoder f(z;\\theta), also sometimes called a \n",
    "#       \"recognition network\". f(.) maps its inputs X onto a normal \n",
    "#       distribution in latent space. Returns the parameters of that \n",
    "#       normal distribution, namely N(z_mean, z_log_sigma_sq) (mod the log, \n",
    "#       see Appendex F.1 of https://arxiv.org/pdf/1312.6114v10.pdf. \n",
    "# \n",
    "#       Note self.transfer_fct = tf.nn.softplus (log(exp(features)+1), \n",
    "#       a smooth approximation to the rectifier function (f(x) = max(0,x)) \n",
    "# \n",
    "    def _f(self, weights, biases): \n",
    "        layer_1        = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']),biases['b1']))  \n",
    "        layer_2        = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']),biases['b2']))  \n",
    "        z_mean         = tf.add(tf.matmul(layer_2, weights['out_mean']),biases['out_mean']) \n",
    "        z_log_sigma_sq = tf.add(tf.matmul(layer_2, weights['out_log_sigma']),biases['out_log_sigma']) \n",
    "        return (z_mean, z_log_sigma_sq) \n",
    "\n",
    "# \n",
    "#       _g \n",
    "# \n",
    "#       Probabilistic decoder (decoder network), which maps points \n",
    "#       in latent space onto a Bernoulli distribution in data space. \n",
    "#       Again, transformation is parametrized and can be learned. \n",
    "# \n",
    "    def _g(self, weights, biases): \n",
    "        layer_1         = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']),biases['b1']))  \n",
    "        layer_2         = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))  \n",
    "        x_reconstr_mean = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), biases['out_mean'])) \n",
    "        return x_reconstr_mean \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#       _elbo\n",
    "# \n",
    "#       The approach taken by VAE's s is to introduce an inference \n",
    "#       model q_{\\phi|(z|x) that learns to  approximate the \n",
    "#       intractable posterior p_{\\theta}(z|x) by optimizing the \n",
    "#       variational lower bound or ELBO (see \n",
    "#       https://arxiv.org/pdf/1601.00670.pdf for details). \n",
    "# \n",
    "#       In this case, the ELBO (loss) is composed of two terms: \n",
    "# \n",
    "#       (i).    The reconstruction loss (the negative log probability  \n",
    "#               of the input under the reconstructed Bernoulli distribution  \n",
    "#               induced by the decoder in the data space). \n",
    "# \n",
    "#               See appendix C.1 of https://arxiv.org/pdf/1312.6114v10.pdf \n",
    "# \n",
    "#               Note: 1e-10 is added to avoid trying to evaluate  log(0.0) \n",
    "# \n",
    "#       (ii).   The latent loss, which is defined as the Kullback Leibler \n",
    "#               Divergence between the distribution in latent space induced \n",
    "#               by the encoder on the data and some prior. \n",
    "# \n",
    "#               The KL term is essentially a regularizer which pulls the \n",
    "#               estimated posterior q_{\\phi}(z|x) toward the prior p(z). \n",
    "# \n",
    "#               See appendix B of https://arxiv.org/pdf/1312.6114v10.pdf \n",
    "# \n",
    "# \n",
    "    def _elbo(self): \n",
    "        reconstr_loss  = -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean) \n",
    "                                        + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean), \n",
    "                                        1) \n",
    "        latent_loss    = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq  \n",
    "                                              - tf.square(self.z_mean)  \n",
    "                                              - tf.exp(self.z_log_sigma_sq), \n",
    "                                              1) \n",
    "        self.cost      = tf.reduce_mean(reconstr_loss + latent_loss)  \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost) \n",
    "         \n",
    "# \n",
    "#       Train on minibatch if desired \n",
    "# \n",
    "    def partial_fit(self, X): \n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost),feed_dict={self.x: X}) \n",
    "        return cost \n",
    "     \n",
    "# \n",
    "#       transform(.) maps to mean of distribution, Note here \n",
    "#       that we could abandon the encoder pathway f(z;\\phi}) and \n",
    "#       simply sample from N(0,1) \n",
    "# \n",
    "    def transform(self, X): \n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\" \n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X}) \n",
    "     \n",
    "# \n",
    "#       generate(.) \n",
    "# \n",
    "#       Generate samples from the latent space.  \n",
    "#         \n",
    "#       Here if z_mu is not None, then the data for this point in \n",
    "#       latent space is generated. Otherwise, z_mu is drawn from prior \n",
    "#       in latent space (i.e., N(0,1)) \n",
    " \n",
    "    def generate(self, z_mu=None): \n",
    "        if z_mu is None: \n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"]) \n",
    "        return self.sess.run(self.x_reconstr_mean,feed_dict={self.z: z_mu}) \n",
    "#     \n",
    "#       reconstruct(.) \n",
    "# \n",
    "#       Reconstruct X \n",
    "# \n",
    "# \n",
    "    def reconstruct(self, X): \n",
    "        return self.sess.run(self.x_reconstr_mean,feed_dict={self.x: X})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       Now, train the vae in mini-batches \n",
    "# \n",
    "# \n",
    "def train(network_architecture, learning_rate=0.001,batch_size=100, training_epochs=10, display_step=5): \n",
    "    vae = VariationalAutoencoder(network_architecture,learning_rate=learning_rate,batch_size=batch_size) \n",
    "    for epoch in range(training_epochs):                       # training cycle \n",
    "        avg_cost    = 0.0\n",
    "        total_batch = int(n_samples / batch_size) \n",
    "        for i in range(total_batch):                           # loop over all batches \n",
    "            batch_xs, _ = mnist.train.next_batch(batch_size) \n",
    "            cost        = vae.partial_fit(batch_xs)            # fit training data \n",
    "            avg_cost   += cost / n_samples * batch_size        # compute average loss \n",
    " \n",
    "        if epoch % display_step == 0:                         \n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost) \n",
    "    return vae "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build an instance. First build a network architecture (network_architecture), then train the network on MNIST. Put a cell below this to hold training_epochs so we can experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epochs=1\n",
    "display_step=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "#       Train a VAE on MNIST. First, specify the network  \n",
    "# \n",
    "network_architecture = dict(n_hidden_f_1=500,       # 1st layer encoder neurons \n",
    "                            n_hidden_f_2=500,       # 2nd layer encoder neurons \n",
    "                            n_hidden_g_1=500,       # 1st layer decoder neurons \n",
    "                            n_hidden_g_2=500,       # 2nd layer decoder neurons \n",
    "                            n_input=784,            # MNIST (img shape: 28*28) \n",
    "                            n_z=20)                 # dimension of latent space (I used 100 on my autoencoder)\n",
    "# \n",
    "#       now train the network \n",
    "# \n",
    "if (DEBUG): \n",
    "        print \"Training for {} epoch(s)\".format(training_epochs) \n",
    "        print \"display_step = {}\".format(display_step)\n",
    "        \n",
    "vae           = train(network_architecture,training_epochs=training_epochs,display_step=display_step) \n",
    "x_sample      = mnist.test.next_batch(100)[0] \n",
    "x_reconstruct = vae.reconstruct(x_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the reconstruction probability. See e.g., algorithm 4 of http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruction_error = mean_squared_error(x_sample, x_reconstruct) \n",
    "print \"reconstruction error = {}\".format(reconstruction_error) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a simple look at what we built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12)) \n",
    "for i in range(5): \n",
    "    plt.subplot(5, 2, 2*i + 1) \n",
    "    plt.imshow(x_sample[i].reshape(28, 28), vmin=0, vmax=1) \n",
    "    plt.title(\"Test input\") \n",
    "    plt.colorbar() \n",
    "    plt.subplot(5, 2, 2*i + 2) \n",
    "    plt.imshow(x_reconstruct[i].reshape(28, 28), vmin=0, vmax=1) \n",
    "    plt.title(\"Reconstruction\") \n",
    "    plt.colorbar() \n",
    "plt.tight_layout() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
